{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-arm Bandits : Stochastic and Adversarial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general multi-arm bandit problem can be expressed as follow :\n",
    "\n",
    " - We are in front of $n$ players, at each time $t$ we will choose a player : let's call it $\\pi_t \\in (1, ...,n)$, and get a reward (that can be either positive or negative) from this player let's note it $X_{\\pi_t} \\in \\mathbb{R}$. \n",
    "\n",
    "\n",
    " - Our goal is to find the best strategy $\\pi$ in order to maximize the expected profit over time : $\\mathbb{E} [ \\sum_{t=1}^{\\infty} X_{\\pi_t} ]$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of selecting the best strategy over time depends a lot on the kind of players we are dealing with, two cases are to consider : \n",
    " \n",
    "$\\star$ Stochastic players : in this case the reward from each player does not depend on our strategy, one should determine which player gives the best reward on average. An example is the machines in a casino, no matter how we are playing we will not influence the output of the machines.\n",
    "  \n",
    "  \n",
    "$\\star$ Game theory problem : The players adapats the reward following our strategy $\\pi$, in this set up if we select a player multiple times, he could change his output strategy and starts outputing very bad rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Stochastic Multi-arm Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will consider the stochastic set up, the output of each player does not depend on the strategy that we are taking.\n",
    "\n",
    "In this case the reward of each player can be expressed as a random variable : $X_i$ with $i \\in (1,...,n)$. We note $\\mu_i$ the mean of the r.v $X_i$ : $\\mu_i = \\mathbb{E}[X_i] $, \n",
    "\n",
    "Finding the best strategy corresponds to determine the arm that gives the highest reward on average $\\mu^* = \\max_{i} \\mu_i$ as soon as possible. In order to judge our algorithm we introduce the regret defined as :\n",
    "\n",
    "$$ R_T = \\sum_{t=1..T} \\mu^* - \\mu_{\\pi_t} = T \\mu^* - \\sum_{t=1..T} \\mu_{\\pi_t}$$\n",
    "\n",
    "\n",
    "The mathematical analysis of the algorithms aims to find an upper bound and lower bound of the Regret.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "\n",
    "### Definition of environment that will similuate the rewards of the players \n",
    "def Get_StoEnv_normal(num_arms=4, mu=[1, 2.1, 2.2, 2.7], sigma=[0.2,0.2, 0.3, 0.1],\n",
    "                     action=0, visibility=True):\n",
    "    \n",
    "    #Safety check :\n",
    "    if num_arms != len(mu) or num_arms != len(sigma):\n",
    "        print('The number of arms selected needs to be equal to the number of parameters, \\\n",
    "              please doucle-check your inputs')\n",
    "        return\n",
    "    \n",
    "    arms = []\n",
    "    for i in range(num_arms):\n",
    "        arms.append(npr.normal(mu[i], sigma[i]))\n",
    "    \n",
    "    if visibility :   \n",
    "        return arms\n",
    "    \n",
    "    else :\n",
    "        return arms[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a complete visibility setup :  [1.0319868360745694, 2.17307010194893, 2.3622009271343525, 2.771940978961101]\n",
      "In a blind visibility setup :  1.4548840615002716\n"
     ]
    }
   ],
   "source": [
    "# Example of the output\n",
    "\n",
    "# Complete visibility :\n",
    "complete_visibility = Get_StoEnv_normal()\n",
    "print('In a complete visibility setup : ', complete_visibility)\n",
    "# Blind visibility :\n",
    "Blind = Get_StoEnv_normal(action=0, visibility=False)\n",
    "print('In a blind visibility setup : ', Blind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stochastic bandits : Complete Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\star$ Complete visibility : At each time $t$ we will select a player $\\pi_t$ and we get access to the rewards of all the players : $X_{t,i}$ with $i \\in (1,...,n)$\n",
    "\n",
    "At each time $t$, the access to the previous rewards allows us to construct an unbaised estimation of the expected return for all the players $\\widehat{\\mu}$ : \n",
    "\n",
    "$$\\widehat{\\mu}_{t,i} = \\frac{1}{t-1} \\sum_{j=1..t-1} X_{j,i} \\rightarrow \\mu_i  $$\n",
    "\n",
    "Our strategy will be to choose the arm with the highest estimation $\\widehat{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def StochasticBandits_CompleteVisibility(num_arms, Time_limit):\n",
    "    \n",
    "    #_______ First define the array with the mean estimates\n",
    "    # As we the rescaling 1/(t-1) doesn't change our stratey we fill this array with \\sum_{j=1..t-1} X_{j,i}\n",
    "    accumulated_reward = np.zeros(num_arms)\n",
    "    \n",
    "    \n",
    "    #_______ First step : arbitrary choice\n",
    "    selected_arm = npr.randint(num_arms)\n",
    "    env = Get_StoEnv_normal(num_arms)\n",
    "    # update the mean estimate\n",
    "    accumulated_reward += env   \n",
    "    #store the env experiences\n",
    "    Historical_env = pd.DataFrame(env,\n",
    "                     index=['arm_1', 'arm_2', 'arm_3', 'arm_4'])\n",
    "    #store the action taken by the algorithm\n",
    "    Historical_arm = pd.DataFrame([selected_arm],\n",
    "                     index=['arm selected'])\n",
    "  \n",
    "    #_______ Second use the estimates at each time transition\n",
    "    for t in np.arange(1,Time_limit):       \n",
    "        #Choose the arm with the highest estimate :\n",
    "        selected_arm = np.argmax(accumulated_reward)\n",
    "        #Interact with the players :\n",
    "        env = Get_StoEnv_normal(num_arms)\n",
    "        #update the mean estimate\n",
    "        accumulated_reward += env        \n",
    "        #Store the exprience in the dataframe\n",
    "        Historical_env[t] = pd.Series(env, index=Historical_env.index)\n",
    "        #store the action taken by the algorithm\n",
    "        Historical_arm[t] = pd.Series([selected_arm], index=Historical_arm.index)\n",
    "      \n",
    "    print('The best arm is : ', selected_arm)\n",
    "    return Historical_env, Historical_arm, selected_arm       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example the arm 4 is the best one with a mean equal to 2.7, our algorithm choose this arm very quickly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best arm is :  3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>arm selected</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  \\\n",
       "arm selected   2   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   \n",
       "\n",
       "              16  17  18  19  \n",
       "arm selected   3   3   3   3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Historical_env, Historical_arm, selected_arm = StochasticBandits_CompleteVisibility(4, 20)\n",
    "Historical_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regret Upper-bound :\n",
    "\n",
    "For 2 players $X_1, X_2 \\in [0,1]$, the upper bound can be expressed as :\n",
    "\n",
    "$$ \\mathbb{E}[R_T] \\leq \\frac{\\delta}{2} + \\frac{1}{\\delta} $$\n",
    "\n",
    "With $\\delta = \\mu_1 - \\mu_2$ where we consider without any loss of generality $\\mu_2 \\leq \\mu_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stochastic bandits : No Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\star$ In this case we only have access to the reward of the selected arm $X_{t, \\pi_{t}}$ :\n",
    "\n",
    "The new estimator : $ \\widehat{\\mu}_{t,i} = \\frac{1}{N_i(t)} \\sum_{j=1..t-1} X_{j,i}*\\mathbb{1}_{\\pi_{j}=i}$ is baised and can not lead to the best arm ! $N_i(t)$ is the number of times the arm $i$ has been selected.\n",
    "\n",
    "In order to deal with this situation, the proposed approach constructs confidence intervals in order to get to the best arm, we will discuss two algorithms UCB : Upper Confidence Interval and ETC : Explore then commit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [Concentration inequality](https://en.wikipedia.org/wiki/Concentration_inequality), we can construct the upper bound of the confidence intervals as :\n",
    "\n",
    "$$ S_{i}(t) = \\widehat{\\mu}_{i}(t) + \\sqrt{\\frac{2 log(t)}{N_i(t)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCB : Upper Confidence Interval :\n",
    "\n",
    "At each time we consider the arm with the highest confidence interval upper bound\n",
    "\n",
    "\n",
    "\n",
    "The regret of the algorithm can be expressed as :\n",
    "\n",
    "$$ \\mathbb{E}[R_T] \\leq  \\sum_{i \\neq *} 8 \\frac{log(T}{\\delta_i} + \\delta_i (\\frac{\\pi^2}{3}+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UCB(num_arms, Time_limit):\n",
    "    \n",
    "    #_______ First store cumulated rewards\n",
    "    # and the numer of times we select an arm\n",
    "    accumulated_reward = np.zeros(num_arms)\n",
    "    arms_count = np.zeros(num_arms)\n",
    "    Hist_selected_arms = []\n",
    "    #______ First select each arm one time :\n",
    "    time = 0\n",
    "    while time < num_arms:\n",
    "        \n",
    "        selected_arm = time\n",
    "        Hist_selected_arms.append(selected_arm)\n",
    "        rew = Get_StoEnv_normal(action=selected_arm, visibility=False)\n",
    "        accumulated_reward[selected_arm] += rew\n",
    "        arms_count[selected_arm] += 1\n",
    "        time += 1\n",
    "        \n",
    "    #safety check :        \n",
    "    if (arms_count==np.ones(num_arms)).all() == False :            \n",
    "        print('error in the first loop')\n",
    "        return\n",
    "        \n",
    "    #___ Start UCB :   \n",
    "    while time <= Time_limit:\n",
    "        \n",
    "        # construct the upper confidence interval for each arm :\n",
    "        UCB = [accumulated_reward[i]/arms_count[i] + np.sqrt(2*np.log(time)/arms_count[i]) for i in range(num_arms)]\n",
    "        #select action with best UCB\n",
    "        selected_arm = np.argmax(UCB)\n",
    "        Hist_selected_arms.append(selected_arm)\n",
    "        #get the reward\n",
    "        rew = Get_StoEnv_normal(action=selected_arm, visibility=False)\n",
    "        #update the cumulative reward and \n",
    "        accumulated_reward[selected_arm] += rew\n",
    "        arms_count[selected_arm] += 1\n",
    "        time += 1\n",
    "    \n",
    "    print('the best arm is the arm : ', selected_arm)\n",
    "    return Hist_selected_arms, selected_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best arm is the arm :  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hist_selected_arms, selected_arm = UCB(4, 1000)\n",
    "selected_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that the algorithm need ~1000 iterations to get the best arm (number 3) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETC : Explore then commit :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETC uses the confidence intervals in a different way than UCB. The main idea is to divide time into lapse, in each lapse we will explore all the available arms, until one of them is clearly not optimal.\n",
    "\n",
    "We will stop considering an arm $i$ if when there is at least one arm $j$ such as :\n",
    "\n",
    "    \n",
    "$$\\widehat{\\mu}_{i}(t) + \\sqrt{\\frac{2 log(t)}{N_i(t)}} \\leq \\widehat{\\mu}_{j}(t) - \\sqrt{\\frac{2 log(t)}{N_j(t)}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ETC(num_arms):\n",
    "    \n",
    "    #_______ First store cumulated rewards\n",
    "    # and the numer of times we select an arm\n",
    "    accumulated_reward = np.zeros(num_arms)\n",
    "    arms_count = np.zeros(num_arms)\n",
    "    Hist_selected_arms = []\n",
    "    #_______ First select each arm one time :\n",
    "    time = 0\n",
    "    while time < num_arms:\n",
    "        \n",
    "        selected_arm = time\n",
    "        Hist_selected_arms.append(selected_arm)\n",
    "        rew = Get_StoEnv_normal(action=selected_arm, visibility=False)\n",
    "        accumulated_reward[selected_arm] += rew\n",
    "        arms_count[selected_arm] += 1\n",
    "        time += 1\n",
    "        \n",
    "    #safety check :        \n",
    "    if (arms_count==np.ones(num_arms)).all() == False :            \n",
    "        print('error in the first loop')\n",
    "        return\n",
    "        \n",
    "    #_______ Start ETC :   \n",
    "    Available_arms = list(range(num_arms))\n",
    "    \n",
    "    while len(Available_arms) > 1:\n",
    "        \n",
    "        #select all the available arms and update accumulated_reward and arms_count :\n",
    "        for arm in Available_arms:\n",
    "            selected_arm = arm\n",
    "            rew = Get_StoEnv_normal(action=selected_arm, visibility=False)\n",
    "            accumulated_reward[selected_arm] += rew\n",
    "            arms_count[selected_arm] += 1\n",
    "        \n",
    "        #Get confidence Interval for the remaining arms :\n",
    "        CI = np.array([ [i, accumulated_reward[i]/arms_count[i] - np.sqrt(2*np.log(time)/arms_count[i]), \\\n",
    "                accumulated_reward[i]/arms_count[i] + np.sqrt(2*np.log(time)/arms_count[i]) ] \\\n",
    "                for i in  Available_arms])\n",
    "        \n",
    "        #get maximum of the lower bound in CI :\n",
    "        max_LCI = np.max(CI[:,1])\n",
    "        \n",
    "        #delete the arms with the Uppet bound less than maximum of the lower bound in CI\n",
    "        for i in range(np.shape(CI)[0]):\n",
    "            if CI[i,2] < max_LCI :\n",
    "                Available_arms.remove(CI[i,0])\n",
    "                    \n",
    "        #update time :\n",
    "        time += 1\n",
    "            \n",
    "    print('the best arm is the arm : ', selected_arm)\n",
    "    return selected_arm, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best arm is the arm :  3\n",
      "time :  195\n",
      "selected_arm :  3\n"
     ]
    }
   ],
   "source": [
    "selected_arm, time = ETC(4)\n",
    "print('time : ', time)\n",
    "print('selected_arm : ', selected_arm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regret upper-bound :\n",
    "\n",
    "$$ \\mathbb{E}[R_T] \\leq  \\sum_{i \\neq *} \\frac{log(T \\delta_i^2)}{\\delta_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
